
# BigData-Hadoop-Spark


This repository contains two real-world data analysis problems, solved using both:
- **Hadoop MapReduce (Python + Streaming)**
- **Apache Spark (PySpark)**

These problems simulate tasks a data engineer or analyst may encounter when working with large-scale log files from e-commerce platforms and web servers.

---

## ğŸ“ Folder Structure

\`\`\`plaintext
hadoop/
â”œâ”€â”€ conversion-rate/
â”‚   â”œâ”€â”€ mapper.py
â”‚   â””â”€â”€ reducer.py
â””â”€â”€ weblog-analytics/
    â”œâ”€â”€ mapper.py
    â””â”€â”€ reducer.py

spark/
â”œâ”€â”€ conversion_rate_spark.py
â””â”€â”€ weblog_analytics_spark.py
\`\`\`

---

## ğŸ§© Problem 1: E-Commerce Conversion Rate

### ğŸ“Œ Scenario
An e-commerce company, **E-Shop**, records user activities such as \`view\`, \`add_to_cart\`, and \`purchase\`. The management team wants to know the **conversion rate** for each product â€” a metric defined as:

\`\`\`
conversion_rate = number_of_purchases / number_of_views
\`\`\`

> ğŸ’¡ A tip from a warehouse employee suggests that \`TypeC_HDMI\` is overstocked, likely due to a **low conversion rate** â€” this acts as a real-world validator for your result.

---

## ğŸ§© Problem 2: Web Server Log Analytics

### ğŸ“Œ Scenario
You are provided with a server log file (\`wserv_up.txt\`) from a website. Each line represents a user request to a web page such as \`/file_2.html\`, \`/file_8.html\`, etc.

Your task is to:
- Count how many times each file (web page) was visited.
- Identify the most and least visited pages.

> ğŸ’¡ It\'s rumored that \`file_8.html\` is useless to users and is rarely visited â€” this provides a useful hint to verify your output.

---

## ğŸ› ï¸ Hadoop Implementation (Folder: \`hadoop/\`)

This folder uses the **MapReduce programming model**, written in Python and designed for **Hadoop Streaming**.

### ğŸ”¹ \`conversion-rate/\`
- \`mapper.py\`: Emits \`(item, action)\` for each log entry.
- \`reducer.py\`: Counts views and purchases, then computes conversion rates per item.

### ğŸ”¹ \`weblog-analytics/\`
- \`mapper.py\`: Extracts the web page (e.g., \`/file_3.html\`) from each line.
- \`reducer.py\`: Aggregates visits and outputs the total per page.

---

## âš¡ Spark Implementation (Folder: \`spark/\`)

This folder contains the **Apache Spark** (PySpark) versions of the same two problems, suitable for distributed computing with large datasets.

### ğŸ”¹ \`conversion_rate_spark.py\`
- Reads the e-commerce log file.
- Parses each line and aggregates view/purchase counts using RDD or DataFrame operations.
- Computes and displays the conversion rate for each item.

### ğŸ”¹ \`weblog_analytics_spark.py\`
- Reads the web server log file.
- Extracts file paths and counts occurrences using Spark transformations.
- Displays sorted counts of page visits.

---




